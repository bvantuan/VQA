{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efad5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "import time\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from prettytable import PrettyTable\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c09317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b18a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Dropout, Embedding, concatenate, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing import text, image, sequence\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6463d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tf version: 2.5.0\n",
      "Using numpy version: 1.19.2\n"
     ]
    }
   ],
   "source": [
    "print('Using tf version: {}'.format(tf.__version__))\n",
    "print('Using numpy version: {}'.format(np.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3bae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder of data\n",
    "DATA_FOLDER = '../content'\n",
    "# folder of processed data\n",
    "PROCESSED_DATA_FOLDER = '../content/processed_data'\n",
    "# folder of dataset evaluation\n",
    "EVALUATION_DATA_FOLDER = '../content/evaluation'\n",
    "# path image format\n",
    "IMAGE_FORMAT = '../content/%s/COCO_%s_%012d.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d96e3",
   "metadata": {},
   "source": [
    "# Preparing the data matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b123b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading from disk\n",
    "with open(f'{PROCESSED_DATA_FOLDER}/vqa_raw_trainVal2014_top1000.json', 'rb') as f:\n",
    "    clQuestionTrainVal, clAnswerTrainVal, clAnswersTrainVal, clPathImageTrainVal = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc9a7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk : question tokenization class\n",
    "with open(f'{DATA_FOLDER}/question_tokenizer.pkl', 'rb') as f:\n",
    "    oQuestionTok = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c64027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk : Tokenized questions\n",
    "with open(f'{DATA_FOLDER}/tokenised_question_paddingPost_train_val.pkl', mode='rb') as f:\n",
    "    caQuestionTrainValTokenized = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c9547a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk: Encode target answers with value between 0 and n_classes-1\n",
    "with open(f'{DATA_FOLDER}/AnswerEncoderTrainVal.pkl', 'rb') as f:\n",
    "    oAnswerEncoder = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4a14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function name: GetAnswersMatrix\n",
    "    \n",
    "    Objective: One-hot-encode the answers\n",
    "    \n",
    "    Summary algorithmic description: Transform answers to normalized encoding.\n",
    "                                     Convert the normalized encoding to one-hot-coding\n",
    "    \n",
    "    Input parameters: clAnswer : list of answers\n",
    "                      cEncoder : LabelEncoder class\n",
    "    \n",
    "    Return : binary class matrix\n",
    "    \n",
    "    Date : 09/11/2021\n",
    "    \n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "def GetAnswersMatrix(clAnswer, cEncoder):\n",
    "    # Transform labels to normalized encoding.\n",
    "    y = cEncoder.transform(clAnswer) \n",
    "    # Number of classes \n",
    "    iNbClasses = cEncoder.classes_.shape[0]\n",
    "    # Convert a class vector (integers) to binary class matrix.\n",
    "    Y = utils.to_categorical(y, iNbClasses)\n",
    "    # Return the binary class matrix.\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2de741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot-encode the train answers \n",
    "caAnswerMatrix = GetAnswersMatrix(clAnswerTrainVal, oAnswerEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1162d333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(574913, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of answer matrix\n",
    "caAnswerMatrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697cbd61",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70cb9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "705af5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Class name: AttentionMaps \n",
    "    \n",
    "    Objective: Compute the image (or question) attention\n",
    "    \n",
    "    Summary algorithmic description: Calculate the affinity matrix\n",
    "                                     Predict image and question attention maps\n",
    "    \n",
    "    Input parameters: iDim_k : hidden attention dimention\n",
    "                      fRegValue : Regularization value\n",
    "    \n",
    "    Date : 14/11/2021\n",
    "    \n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "class AttentionMaps(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Function name: __init__\n",
    "\n",
    "        Objective: Define custom layer attributes\n",
    "\n",
    "        Summary algorithmic description: Define two attributes hidden attention dimention and regularization value\n",
    "                                         Define the weight parameters by layer Dense\n",
    "\n",
    "        Input parameters: iDim_k : hidden attention dimention\n",
    "                          fRegValue : Regularization value\n",
    "\n",
    "        Return : None\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def __init__(self, iDim_k, fRegValue, **kwargs):\n",
    "        # Default layer attributes\n",
    "        super(AttentionMaps, self).__init__(**kwargs)\n",
    "\n",
    "        # Get the value of hidden attention dimention\n",
    "        self.iDim_k = iDim_k\n",
    "        # Get the regularization value \n",
    "        self.fRegValue = fRegValue\n",
    "\n",
    "        # Dense layer with Xavier uniform initializer for image weight parameters \n",
    "        self.Wv = Dense(self.iDim_k, activation=None,\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.fRegValue),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=2))\n",
    "        # Dense layer with Xavier uniform initializer for question weight parameters\n",
    "        self.Wq = Dense(self.iDim_k, activation=None,\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.fRegValue),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=3))\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "        Function name: call\n",
    "\n",
    "        Objective: Perform the logic of applying the layer to the input tensors\n",
    "\n",
    "        Summary algorithmic description: Calculate the affinity matrix\n",
    "                                         Predict image and question attention maps\n",
    "\n",
    "        Input parameters: caImageFeat : image feature (b, N,  d) \n",
    "                          caQuestionFeat : question feature (b, T,  d)\n",
    "\n",
    "        Return : image attention map and question attention map\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def call(self, caImageFeat, caQuestionFeat):\n",
    "        # Affinity Matrix C = tanh((QT)(Wb)V) (b, T, N)\n",
    "        C = tf.matmul(caQuestionFeat, tf.transpose(caImageFeat, perm=[0,2,1])) \n",
    "        C = tf.keras.activations.tanh(C) \n",
    "\n",
    "        # (Wv)V (b, N, k)\n",
    "        Wv_V = self.Wv(caImageFeat)                             \n",
    "        # (Wq)Q (b, T, k)\n",
    "        Wq_Q = self.Wq(caQuestionFeat)                              \n",
    "\n",
    "        # ((Wq)Q)C (b, k, N)\n",
    "        Wq_Q_C = tf.matmul(tf.transpose(Wq_Q, perm=[0,2,1]), C) \n",
    "        # ((Wq)Q)C (b, N, k)\n",
    "        Wq_Q_C = tf.transpose(Wq_Q_C, perm =[0,2,1])       \n",
    "\n",
    "        # ((Wv)V)CT (b, k, T)                                         \n",
    "        Wv_V_CT = tf.matmul(tf.transpose(Wv_V, perm=[0,2,1]), tf.transpose(C, perm=[0,2,1])) \n",
    "        # ((Wv)V)CT (b, T, k) \n",
    "        Wv_V_CT = tf.transpose(Wv_V_CT, perm =[0,2,1])         \n",
    "\n",
    "        # image attention map Hv = tanh((Wv)V + ((Wq)Q)C) (b, N, k)\n",
    "        Hv = Wv_V + Wq_Q_C                                     \n",
    "        Hv = tf.keras.activations.tanh(Hv)              \n",
    "\n",
    "        # question attention map Hq = tanh((Wq)Q + ((Wv)V)CT) (b, T, k)\n",
    "        Hq = Wq_Q + Wv_V_CT                                     \n",
    "        Hq = tf.keras.activations.tanh(Hq)                   \n",
    "\n",
    "        # Return image attention map and question attention map\n",
    "        return [Hv, Hq]                                     \n",
    "  \n",
    "\n",
    "    \"\"\"\n",
    "        Function name: get_config\n",
    "\n",
    "        Objective: Collect the input shape and other information about the layer.\n",
    "\n",
    "        Summary algorithmic description: Return a dictionary containing the configuration used to initialize this layer\n",
    "\n",
    "        Input parameters: None\n",
    "\n",
    "        Return : a dictionary containing informations about the layer.\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def get_config(self):\n",
    "        # Dictionnary of two attributes of this layer\n",
    "        cdConfig = {\n",
    "            'iDim_k': self.iDim_k,\n",
    "            'fRegValue': self.fRegValue\n",
    "        }\n",
    "        # Dictionnary of base configurations\n",
    "        cdBasaConfig = super(AttentionMaps, self).get_config()\n",
    "        # Return a dictionary containing informations about the layer.\n",
    "        return dict(list(cdBasaConfig.items()) + list(cdConfig.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73f85dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'attention_maps', 'trainable': True, 'dtype': 'float32', 'iDim_k': 64, 'fRegValue': 0.001}\n"
     ]
    }
   ],
   "source": [
    "cLayerAttentionMaps = AttentionMaps(64, 0.001)\n",
    "cdConfig = cLayerAttentionMaps.get_config()\n",
    "print(cdConfig)\n",
    "cNewLayerAttentionMaps = AttentionMaps.from_config(cdConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba8bfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Class name: ContextVector \n",
    "    \n",
    "    Objective: Find context vector of the image and text features\n",
    "    \n",
    "    Summary algorithmic description: \n",
    "    \n",
    "    Input parameters: iDim_k : hidden attention dimention\n",
    "                      fRegValue : Regularization value\n",
    "    \n",
    "    Date : 14/11/2021\n",
    "    \n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "class ContextVector(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Function name: __init__\n",
    "\n",
    "        Objective: Define custom layer attributes\n",
    "\n",
    "        Summary algorithmic description: Define attribute regularization value\n",
    "                                         Define the weight parameters by layer Dense\n",
    "\n",
    "        Input parameters: fRegValue : Regularization value\n",
    "\n",
    "        Return : None\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def __init__(self, fRegValue, **kwargs):\n",
    "        # Default layer attributes\n",
    "        super(ContextVector, self).__init__(**kwargs)\n",
    "\n",
    "        # Get the regularization value \n",
    "        self.fRegValue = fRegValue\n",
    "        # Image weight parameters (Dense layer with Xavier uniform initializer)\n",
    "        self.whv = Dense(1, activation='softmax',\\\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(self.fRegValue),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=4))\n",
    "        # Image weight parameters (Dense layer with Xavier uniform initializer)\n",
    "        self.whq = Dense(1, activation='softmax',\\\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(self.fRegValue),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=5)) \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        Function name: call\n",
    "\n",
    "        Objective: Perform the logic of applying the layer to the input tensors\n",
    "\n",
    "        Summary algorithmic description: Find context vector of the image and text features\n",
    "\n",
    "        Input parameters: caImageFeat : image feature (b, N,  d) \n",
    "                          caQuestionFeat : question feature (b, T,  d)\n",
    "                          Hv : image attention map (b, N, k)\n",
    "                          Hq : question attention map (b, T, k)\n",
    "\n",
    "        Return : context vector of the image and text features\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def call(self, caImageFeat, caQuestionFeat, Hv, Hq):\n",
    "        # attention probabilities of each image region vn\n",
    "        # av = softmax(wT_hv * H_v) (b, N, 1)\n",
    "        av = self.whv(Hv)                               \n",
    "\n",
    "        # attention probabilities of each word qt      \n",
    "        # aq = softmax(wT_hq * H_q) (b, T, 1)\n",
    "        aq = self.whq(Hq)                              \n",
    "\n",
    "        # context vector for image (b, N, d)\n",
    "        v = av * caImageFeat \n",
    "        # context vector for image (b, d)\n",
    "        v = tf.reduce_sum(v, 1)                            \n",
    "\n",
    "        # context vector for question (b, T, d)\n",
    "        q = aq * caQuestionFeat                               \n",
    "        # context vector for question (b, d)\n",
    "        q = tf.reduce_sum(q, 1)            \n",
    "        \n",
    "        # Return context vector of the image and text features\n",
    "        return [v, q]\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "        Function name: get_config\n",
    "\n",
    "        Objective: Collect the input shape and other information about the layer.\n",
    "\n",
    "        Summary algorithmic description: Return a dictionary containing the configuration used to initialize this layer\n",
    "\n",
    "        Input parameters: None\n",
    "\n",
    "        Return : a dictionary containing informations about the layer.\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def get_config(self):\n",
    "        # Dictionnary of attributes of this layer\n",
    "        cdConfig = {\n",
    "            'fRegValue': self.fRegValue\n",
    "        }\n",
    "        # Dictionnary of base configurations\n",
    "        cdBaseConfig = super(ContextVector, self).get_config()\n",
    "        # Return a dictionary containing informations about the layer\n",
    "        return dict(list(cdBaseConfig.items()) + list(cdConfig.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a6f9ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'context_vector', 'trainable': True, 'dtype': 'float32', 'fRegValue': 0.001}\n"
     ]
    }
   ],
   "source": [
    "cLayerContextVector = ContextVector(0.001)\n",
    "cdConfig = cLayerContextVector.get_config()\n",
    "print(cdConfig)\n",
    "cNewLayerContextVector = ContextVector.from_config(cdConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8501c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Class name: PhraseLevelFeatures \n",
    "    \n",
    "    Objective: Compute the phrase features by applying 1-D convolution on the word embedding vectors\n",
    "    \n",
    "    Summary algorithmic description: Applying 1-D convolution with filters of three window sizes: unigram, bigram and trigram\n",
    "                                     Max-pooling across different n-grams at each word location\n",
    "    \n",
    "    Input parameters: iDim_d : hidden dimension\n",
    "    \n",
    "    Date : 14/11/2021\n",
    "    \n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "class PhraseLevelFeatures(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Function name: __init__\n",
    "\n",
    "        Objective: Define custom layer attributes\n",
    "\n",
    "        Summary algorithmic description: Define attribute hidden dimension\n",
    "                                         Define the weight parameters by 1D convolution layer\n",
    "\n",
    "        Input parameters: iDim_d : hidden dimension\n",
    "\n",
    "        Return : None\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def __init__(self, iDim_d, **kwargs):\n",
    "        # Default layer attributes\n",
    "        super(PhraseLevelFeatures, self).__init__(**kwargs)\n",
    "\n",
    "        # Get the hidden dimension\n",
    "        self.iDim_d = iDim_d\n",
    "        # Convolution with filter unigram\n",
    "        self.conv_unigram = Conv1D(self.iDim_d, kernel_size=1, strides=1,\\\n",
    "                                    kernel_initializer=tf.keras.initializers.glorot_uniform(seed=6)) \n",
    "        # Convolution with filter bigram\n",
    "        self.conv_bigram =  Conv1D(self.iDim_d, kernel_size=2, strides=1, padding='same',\\\n",
    "                                    kernel_initializer=tf.keras.initializers.glorot_uniform(seed=7)) \n",
    "        # Convolution with filter trigram\n",
    "        self.conv_trigram = Conv1D(self.iDim_d, kernel_size=3, strides=1, padding='same',\\\n",
    "                                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=8)) \n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "        Function name: call\n",
    "\n",
    "        Objective: Perform the logic of applying the layer to the input tensors\n",
    "\n",
    "        Summary algorithmic description: Compute the n-gram phrase embeddings (n = 1,2,3)\n",
    "\n",
    "        Input parameters: caWordFeat : word feature (b, T, d) \n",
    "\n",
    "        Return : Phrase level features of the question (b, T, d)\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def call(self, caWordFeat):\n",
    "        # phrase level unigram features (b, T, d)\n",
    "        qpUnigram = self.conv_unigram(caWordFeat)                    \n",
    "\n",
    "        # phrase level bigram features (b, T, d)\n",
    "        qpBigram  = self.conv_bigram(caWordFeat)                     \n",
    "\n",
    "        # phrase level trigram features (b, T, d)\n",
    "        qpTrigram = self.conv_trigram(caWordFeat)             \n",
    "\n",
    "        # phrase level features (b, T, d, 3)\n",
    "        qp = tf.concat([tf.expand_dims(qpUnigram, -1),\\\n",
    "                        tf.expand_dims(qpBigram, -1),\\\n",
    "                        tf.expand_dims(qpTrigram, -1)], -1)         \n",
    "\n",
    "        # Max-pool across n-gram features, over-all phrase level feature (b, T, d)\n",
    "        qp = tf.reduce_max(qp, -1)                                \n",
    "\n",
    "        # Return phrase level features\n",
    "        return qp\n",
    "\n",
    "    \"\"\"\n",
    "        Function name: get_config\n",
    "\n",
    "        Objective: Collect the input shape and other information about the layer.\n",
    "\n",
    "        Summary algorithmic description: Return a dictionary containing the configuration used to initialize this layer\n",
    "\n",
    "        Input parameters: None\n",
    "\n",
    "        Return : a dictionary containing informations about the layer.\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def get_config(self):\n",
    "        # Dictionnary of attributes of this layer\n",
    "        cdConfig = {\n",
    "            'iDim_d': self.iDim_d\n",
    "        }\n",
    "        # Dictionnary of base configurations\n",
    "        cdBaseConfig = super(PhraseLevelFeatures, self).get_config()\n",
    "        # Return a dictionary containing informations about the layer\n",
    "        return dict(list(cdBaseConfig.items()) + list(cdConfig.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22e70939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'phrase_level_features', 'trainable': True, 'dtype': 'float32', 'iDim_d': 32}\n"
     ]
    }
   ],
   "source": [
    "cLayerPhraseLevelFeatures = PhraseLevelFeatures(32)\n",
    "cdConfig = cLayerPhraseLevelFeatures.get_config()\n",
    "print(cdConfig)\n",
    "cNewLayerPhraseLevelFeatures = PhraseLevelFeatures.from_config(cdConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bfcab",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d6c2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function name: BuildModel\n",
    "\n",
    "    Objective: build the training model\n",
    "\n",
    "    Summary algorithmic description: Calculate co-attended image and question features from all three levels\n",
    "                                     Use a multi-layer perceptron (MLP) to recursively encode the attention features\n",
    "    \n",
    "    Input parameters: iMaxAnswers : Number of output targets of the model\n",
    "                      iMaxSequenceLength : Maximum length of input sequences\n",
    "                      iVocabSize : Size of the vocabulary\n",
    "                      iDim_d : Hidden dimension\n",
    "                      iDim_k : Hidden attention dimension\n",
    "                      fLearningRate : Learning rate for the model\n",
    "                      fDropoutRate : Dropout rate\n",
    "                      fRegValue : Regularization value\n",
    "\n",
    "    Return : The training model\n",
    "\n",
    "    Date : 14/11/2021\n",
    "\n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "def BuildModel(iMaxAnswers, iMaxSequenceLength, iVocabSize, iDim_d, iDim_k, fLearningRate, fDropoutRate, fRegValue):\n",
    "    # inputs \n",
    "    caImageInput = Input(shape=(49, 512, ), name='Image_Input')\n",
    "    caQuestionInput = Input(shape=(22, ), name='Question_Input')\n",
    "\n",
    "    # image feature (Wb)V (b, N, d)\n",
    "    caImageFeature = Dense(iDim_d, activation=None, name='Image_Feat_Dense',\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(fRegValue),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=1))(caImageInput)\n",
    "    # image feature (b, N, d)\n",
    "    caImageFeature = Dropout(fDropoutRate, seed=1)(caImageFeature)\n",
    "\n",
    "    # word embedding feature\n",
    "    caWordFeature = Embedding(input_dim=iVocabSize, output_dim=iDim_d, input_length=iMaxSequenceLength,\\\n",
    "                            mask_zero=True)(caQuestionInput)\n",
    "    # image and question attention maps at word level\n",
    "    Hvw, Hqw = AttentionMaps(iDim_k, fRegValue, name='AttentionMaps_Word')(caImageFeature, caWordFeature)\n",
    "    # context vector of the image and text features at word level\n",
    "    vw, qw = ContextVector(fRegValue, name='ContextVector_Word')(caImageFeature, caWordFeature, Hvw, Hqw)\n",
    "    # attention features at word level\n",
    "    caWordAttentionFeature = tf.add(vw, qw)\n",
    "    # co-attended image and question features from word level\n",
    "    hw = Dense(iDim_d, activation='tanh', name='hw_Dense',\\\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(fRegValue),\\\n",
    "                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=13))(caWordAttentionFeature)\n",
    "\n",
    "    # phrase level features\n",
    "    caPhraseFeature = PhraseLevelFeatures(iDim_d, name='PhraseLevelFeatures')(caWordFeature)\n",
    "    # image and question attention maps at phrase level\n",
    "    Hvp, Hqp = AttentionMaps(iDim_k, fRegValue, name='AttentionMaps_Phrase')(caImageFeature, caPhraseFeature)\n",
    "    # context vector of the image and text features at phrase level\n",
    "    vp, qp = ContextVector(fRegValue, name='ContextVector_Phrase')(caImageFeature, caPhraseFeature, Hvp, Hqp)\n",
    "    # attention features at phrase level\n",
    "    caPhraseAttentionFeature = concatenate([tf.add(vp, qp), hw], -1) \n",
    "    # co-attended image and question features from phrase level\n",
    "    hp = Dense(iDim_d, activation='tanh', name='hp_Dense',\\\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(fRegValue),\\\n",
    "                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=14))(caPhraseAttentionFeature)\n",
    "\n",
    "    # sentence level\n",
    "    caSentenceFeature = LSTM(iDim_d, return_sequences=True, input_shape=(None, iMaxSequenceLength, iDim_d),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(caPhraseFeature)\n",
    "    # image and question attention maps at sentence level\n",
    "    Hvs, Hqs = AttentionMaps(iDim_k, fRegValue, name='AttentionMaps_Sent')(caImageFeature, caSentenceFeature)\n",
    "    # context vector of the image and text features at sentence level\n",
    "    vs, qs = ContextVector(fRegValue, name='ContextVector_Sent')(caImageFeature, caSentenceFeature, Hvs, Hqs)\n",
    "    # attention features at sentence level\n",
    "    caSentenceAttentionFeature = concatenate([tf.add(vs, qs), hp], -1) \n",
    "    # co-attended image and question features from sentence level\n",
    "    hs = Dense(2 * iDim_d, activation='tanh', name='hs_Dense',\\\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(fRegValue),\\\n",
    "                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=15))(caSentenceAttentionFeature)\n",
    "\n",
    "    # encode the attention features\n",
    "    z = Dense(2 * iDim_d, activation='tanh', name='z_Dense',\\\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(fRegValue),\\\n",
    "                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(hs)\n",
    "    # encode the attention features\n",
    "    z = Dropout(fDropoutRate, seed=16)(z)\n",
    "\n",
    "    # probability of the final answer\n",
    "    result = Dense(iMaxAnswers, activation='softmax')(z)\n",
    "    \n",
    "    # group layers\n",
    "    model = Model(inputs=[caImageInput, caQuestionInput], outputs=result)\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2932556",
   "metadata": {},
   "source": [
    "# Create tf.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "181febf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 300\n",
    "# buffer siez\n",
    "BUFFER_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70f30a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function name: GetImageFeatures\n",
    "\n",
    "    Objective: Load the image features from numpy files\n",
    "\n",
    "    Summary algorithmic description: Load the image features from numpy files\n",
    "\n",
    "    Input parameters: sPathImage : path image\n",
    "                      caQuestion : Tokenization of question\n",
    "                      caAnswer : answer one-hot-enconding \n",
    "\n",
    "    Return : caImageTensor : image features\n",
    "             caQuestion : Tokenization of question\n",
    "             caAnswer : answer one-hot-enconding \n",
    "\n",
    "    Date : 15/11/2021\n",
    "\n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "def GetImageFeatures(sPathImage, caQuestion, caAnswer):\n",
    "    # Load the image features from numpy files\n",
    "    caImageTensor = np.load(f'{DATA_FOLDER}/features/' + sPathImage.decode('utf-8').split('.')[2][-6:] + '.npy')\n",
    "    # Return image features, tokenization of question and answer one-hot-enconding \n",
    "    return caImageTensor, caQuestion, caAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b3c8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset whose elements are slices along their first dimension\n",
    "oTrainDataset = tf.data.Dataset.from_tensor_slices((clPathImageTrainVal, caQuestionTrainValTokenized, caAnswerMatrix))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "oTrainDataset = oTrainDataset.map(lambda sPathImage, caQuestion, caAnswer: tf.numpy_function(\n",
    "    GetImageFeatures, [sPathImage, caQuestion, caAnswer], [tf.float32, tf.int32, tf.float32]),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "oTrainDataset = oTrainDataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# allow later elements to be prepared while the current element is being processed.\n",
    "oTrainDataset = oTrainDataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354a0ff",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97702b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params 1\n",
    "MAX_ANSWERS = 1000\n",
    "MAX_SEQUENCE_LENGTH = 22\n",
    "VOCAB_SIZE = len(oQuestionTok.word_index) + 1\n",
    "EPOCHS      = 100\n",
    "DIM_D = 512\n",
    "DIM_K = 256\n",
    "LEARNING_RATE = 1e-4\n",
    "DROPOUT_RATE = 0.5\n",
    "REG_VALUE = 0.01\n",
    "\n",
    "BASE_PATH = f'{DATA_FOLDER}/temps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffdfd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "oModel = BuildModel(MAX_ANSWERS, MAX_SEQUENCE_LENGTH, VOCAB_SIZE, DIM_D, DIM_K, LEARNING_RATE, DROPOUT_RATE, REG_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f3006f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Image_Input (InputLayer)        [(None, 49, 512)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Image_Feat_Dense (Dense)        (None, 49, 512)      262656      Image_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Question_Input (InputLayer)     [(None, 22)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 49, 512)      0           Image_Feat_Dense[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 22, 512)      9207808     Question_Input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "PhraseLevelFeatures (PhraseLeve (None, 22, 512)      1574400     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "AttentionMaps_Word (AttentionMa [(None, 49, 256), (N 262656      dropout[0][0]                    \n",
      "                                                                 embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "AttentionMaps_Phrase (Attention [(None, 49, 256), (N 262656      dropout[0][0]                    \n",
      "                                                                 PhraseLevelFeatures[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ContextVector_Word (ContextVect [(None, 512), (None, 514         dropout[0][0]                    \n",
      "                                                                 embedding[0][0]                  \n",
      "                                                                 AttentionMaps_Word[0][0]         \n",
      "                                                                 AttentionMaps_Word[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 22, 512)      2099200     PhraseLevelFeatures[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ContextVector_Phrase (ContextVe [(None, 512), (None, 514         dropout[0][0]                    \n",
      "                                                                 PhraseLevelFeatures[0][0]        \n",
      "                                                                 AttentionMaps_Phrase[0][0]       \n",
      "                                                                 AttentionMaps_Phrase[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add (TFOpLambda)        (None, 512)          0           ContextVector_Word[0][0]         \n",
      "                                                                 ContextVector_Word[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "AttentionMaps_Sent (AttentionMa [(None, 49, 256), (N 262656      dropout[0][0]                    \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_1 (TFOpLambda)      (None, 512)          0           ContextVector_Phrase[0][0]       \n",
      "                                                                 ContextVector_Phrase[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "hw_Dense (Dense)                (None, 512)          262656      tf.math.add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ContextVector_Sent (ContextVect [(None, 512), (None, 514         dropout[0][0]                    \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 AttentionMaps_Sent[0][0]         \n",
      "                                                                 AttentionMaps_Sent[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           tf.math.add_1[0][0]              \n",
      "                                                                 hw_Dense[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_2 (TFOpLambda)      (None, 512)          0           ContextVector_Sent[0][0]         \n",
      "                                                                 ContextVector_Sent[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "hp_Dense (Dense)                (None, 512)          524800      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1024)         0           tf.math.add_2[0][0]              \n",
      "                                                                 hp_Dense[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hs_Dense (Dense)                (None, 1024)         1049600     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_Dense (Dense)                 (None, 1024)         1049600     hs_Dense[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           z_Dense[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1000)         1025000     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,845,230\n",
      "Trainable params: 17,845,230\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "oModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43906c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of steps per epoch\n",
    "iStepsPerEpoch = int(np.ceil(len(clPathImageTrainVal)/BATCH_SIZE))\n",
    "# interval boundaries for changed learning rates.\n",
    "clBoundary = [50 * iStepsPerEpoch]\n",
    "# the learning rate values for the intervals defined by boundaries.\n",
    "clLearningRateValue = [LEARNING_RATE, LEARNING_RATE / 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d059ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reduce the learning rate after 50th epoch (from 1e-4 to 1e-5)\n",
    "learningRateSchedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(clBoundary, clLearningRateValue)\n",
    "# Optimizer that implements the Adam algorithm\n",
    "oOptimizer = tf.keras.optimizers.Adam(learning_rate=learningRateSchedule)\n",
    "# the crossentropy loss between the labels and predictions\n",
    "oLoss = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b933c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint directory\n",
    "sCheckpointDirectory = BASE_PATH + \"/training_checkpoints/\" + str(LEARNING_RATE) + \"_\" + str(DIM_K)\n",
    "# save checkpoint every SAVE_CKPT_FREQ step\n",
    "SAVE_CKPT_FREQ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1c94ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Checkpoint that will manage three objects with trackable state\n",
    "oCheckpoint = tf.train.Checkpoint(step=tf.Variable(0), optimizer=oOptimizer, model=oModel)\n",
    "# keep only 3 newest checkpoints \n",
    "oCheckPointManager = tf.train.CheckpointManager(oCheckpoint, sCheckpointDirectory, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd0c5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the (weighted) mean of the given values\n",
    "oTrainLoss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2861196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F-1 Score\n",
    "oTrainScore = F1Score(num_classes=MAX_ANSWERS, average='micro', name='train_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1c771fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training log directory\n",
    "sTrainLogDir = BASE_PATH + '/logs/' + str(LEARNING_RATE) + \"_\" + str(DIM_K) + '/train'\n",
    "\n",
    "# Create a summary file writer for the training log directory\n",
    "oTrainSummaryWriter = tf.summary.create_file_writer(sTrainLogDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10516af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function name: TrainStep\n",
    "\n",
    "    Objective: Operate a traning step\n",
    "\n",
    "    Summary algorithmic description: Make a forward pass\n",
    "                                     Make a backward pass\n",
    "                                     Record results : loss mean, F1 score\n",
    "\n",
    "    Input parameters: oModel : training model\n",
    "                      tsImageFeature : image feature tensor\n",
    "                      tsQuestionFeature : question feature tensor\n",
    "                      tsAnswerLabel : answer label tensor\n",
    "                      oOptimizer : Optimizer object\n",
    "\n",
    "    Return : clGradient_ : all gradients against trainable variables\n",
    "\n",
    "    Date : 15/11/2021\n",
    "\n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "def TrainStep(oModel, tsImageFeature, tsQuestionFeature, tsAnswerLabel, oOptimizer):\n",
    "    # Record operations for automatic differentiation\n",
    "    with tf.GradientTape() as oGradientTape:\n",
    "        # forward pass\n",
    "        # résultat de la prédiction\n",
    "        tsPrediction = oModel([tsImageFeature, tsQuestionFeature], training=True)\n",
    "        # the crossentropy loss\n",
    "        fLoss = oLoss(tsAnswerLabel, tsPrediction)\n",
    "\n",
    "    # backward pass\n",
    "    # Compute the gradient of the loss\n",
    "    clGradient = oGradientTape.gradient(fLoss, oModel.trainable_variables)\n",
    "    # Apply gradients to variables\n",
    "    oOptimizer.apply_gradients(zip(clGradient, oModel.trainable_variables))\n",
    "\n",
    "    # record results\n",
    "    # Loss Mean\n",
    "    oTrainLoss(fLoss)\n",
    "    # F-1 Score\n",
    "    oTrainScore(tsAnswerLabel, tsPrediction)\n",
    "\n",
    "    # all gradients\n",
    "    clGradient_ = list(zip(clGradient, oModel.trainable_variables))\n",
    "    # Return all gradients\n",
    "    return clGradient_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66dd09aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch\n"
     ]
    }
   ],
   "source": [
    "# If the prefix of the most recent checkpoint exist\n",
    "if oCheckPointManager.latest_checkpoint:\n",
    "    # Restore the latest checkpoint\n",
    "    oCheckpoint.restore(oCheckPointManager.latest_checkpoint)\n",
    "    # Display the prefix of the most recent checkpoint\n",
    "    print(\"Restored from {}\".format(oCheckPointManager.latest_checkpoint))\n",
    "    # latest training epoch\n",
    "    START_EPOCH = int(oCheckPointManager.latest_checkpoint.split('-')[-1]) * SAVE_CKPT_FREQ\n",
    "    print(\"Resume training from epoch: {}\".format(START_EPOCH))\n",
    "# If the prefix of the most recent checkpoint doesn't exist\n",
    "else:\n",
    "    print(\"Initializing from scratch\")\n",
    "    # Train from scratch\n",
    "    START_EPOCH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "614b48a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 3.6448, f1_score: 0.2434, time: 5982 sec\n",
      "Epoch 2, loss: 2.9653, f1_score: 0.2822, time: 6220 sec\n",
      "Epoch 3, loss: 2.7337, f1_score: 0.3023, time: 6204 sec\n",
      "Epoch 4, loss: 2.6313, f1_score: 0.3165, time: 6193 sec\n",
      "Epoch 5, loss: 2.5186, f1_score: 0.3313, time: 6203 sec\n",
      "Saved checkpoint.\n",
      "Epoch 6, loss: 2.4340, f1_score: 0.3393, time: 6219 sec\n",
      "Epoch 7, loss: 2.3755, f1_score: 0.3447, time: 6236 sec\n",
      "Epoch 8, loss: 2.3334, f1_score: 0.3495, time: 6250 sec\n",
      "Epoch 9, loss: 2.2991, f1_score: 0.3522, time: 6256 sec\n",
      "Epoch 10, loss: 2.2621, f1_score: 0.3559, time: 6349 sec\n",
      "Saved checkpoint.\n",
      "Epoch 11, loss: 2.2334, f1_score: 0.3584, time: 6500 sec\n",
      "Epoch 12, loss: 2.2091, f1_score: 0.3613, time: 6480 sec\n",
      "Epoch 13, loss: 2.1844, f1_score: 0.3646, time: 6531 sec\n",
      "Epoch 14, loss: 2.1602, f1_score: 0.3674, time: 6502 sec\n",
      "Epoch 15, loss: 2.1405, f1_score: 0.3693, time: 6535 sec\n",
      "Saved checkpoint.\n",
      "Epoch 16, loss: 2.1234, f1_score: 0.3720, time: 6538 sec\n",
      "Epoch 17, loss: 2.1075, f1_score: 0.3737, time: 6541 sec\n",
      "Epoch 18, loss: 2.0963, f1_score: 0.3744, time: 6576 sec\n",
      "Epoch 19, loss: 2.0763, f1_score: 0.3765, time: 6549 sec\n",
      "Epoch 20, loss: 2.0602, f1_score: 0.3797, time: 6569 sec\n",
      "Saved checkpoint.\n",
      "Epoch 21, loss: 2.0505, f1_score: 0.3815, time: 6584 sec\n",
      "Epoch 22, loss: 2.0358, f1_score: 0.3832, time: 6599 sec\n",
      "Epoch 23, loss: 2.0234, f1_score: 0.3842, time: 6578 sec\n",
      "Epoch 24, loss: 2.0177, f1_score: 0.3845, time: 6570 sec\n",
      "Epoch 25, loss: 2.0043, f1_score: 0.3866, time: 6583 sec\n",
      "Saved checkpoint.\n",
      "Epoch 26, loss: 1.9921, f1_score: 0.3885, time: 6608 sec\n",
      "Epoch 27, loss: 1.9850, f1_score: 0.3886, time: 6569 sec\n",
      "Epoch 28, loss: 1.9732, f1_score: 0.3903, time: 6572 sec\n",
      "Epoch 29, loss: 1.9601, f1_score: 0.3926, time: 6586 sec\n",
      "Epoch 30, loss: 1.9493, f1_score: 0.3943, time: 6582 sec\n",
      "Saved checkpoint.\n",
      "Epoch 31, loss: 1.9415, f1_score: 0.3960, time: 6588 sec\n",
      "Epoch 32, loss: 1.9293, f1_score: 0.3963, time: 6575 sec\n",
      "Epoch 33, loss: 1.9196, f1_score: 0.3985, time: 6552 sec\n",
      "Epoch 34, loss: 1.9135, f1_score: 0.3999, time: 6580 sec\n",
      "Epoch 35, loss: 1.9038, f1_score: 0.4010, time: 6570 sec\n",
      "Saved checkpoint.\n",
      "Epoch 36, loss: 1.8973, f1_score: 0.4022, time: 6525 sec\n",
      "Epoch 37, loss: 1.8872, f1_score: 0.4026, time: 6578 sec\n",
      "Epoch 38, loss: 1.8806, f1_score: 0.4043, time: 6595 sec\n",
      "Epoch 39, loss: 1.8729, f1_score: 0.4047, time: 6578 sec\n",
      "Epoch 40, loss: 1.8650, f1_score: 0.4061, time: 6564 sec\n",
      "Saved checkpoint.\n",
      "Epoch 41, loss: 1.8597, f1_score: 0.4064, time: 6612 sec\n",
      "Epoch 42, loss: 1.8508, f1_score: 0.4072, time: 6571 sec\n",
      "Epoch 43, loss: 1.8496, f1_score: 0.4079, time: 6598 sec\n",
      "Epoch 44, loss: 1.8463, f1_score: 0.4090, time: 6608 sec\n",
      "Epoch 45, loss: 1.8336, f1_score: 0.4103, time: 6580 sec\n",
      "Saved checkpoint.\n",
      "Epoch 46, loss: 1.8275, f1_score: 0.4114, time: 6585 sec\n",
      "Epoch 47, loss: 1.8207, f1_score: 0.4123, time: 7085 sec\n",
      "Epoch 48, loss: 1.8162, f1_score: 0.4125, time: 6622 sec\n",
      "Epoch 49, loss: 1.8145, f1_score: 0.4129, time: 7071 sec\n",
      "Epoch 50, loss: 1.8066, f1_score: 0.4137, time: 6599 sec\n",
      "Saved checkpoint.\n",
      "Epoch 51, loss: 1.7640, f1_score: 0.4214, time: 6627 sec\n",
      "Epoch 52, loss: 1.7469, f1_score: 0.4252, time: 6616 sec\n",
      "Epoch 53, loss: 1.7370, f1_score: 0.4278, time: 6591 sec\n",
      "Epoch 54, loss: 1.7294, f1_score: 0.4289, time: 7081 sec\n",
      "Epoch 55, loss: 1.7226, f1_score: 0.4311, time: 7109 sec\n",
      "Saved checkpoint.\n",
      "Epoch 56, loss: 1.7178, f1_score: 0.4311, time: 6954 sec\n",
      "Epoch 57, loss: 1.7124, f1_score: 0.4333, time: 7082 sec\n",
      "Epoch 58, loss: 1.7091, f1_score: 0.4332, time: 7101 sec\n",
      "Epoch 59, loss: 1.7049, f1_score: 0.4341, time: 7110 sec\n",
      "Epoch 60, loss: 1.7021, f1_score: 0.4352, time: 7135 sec\n",
      "Saved checkpoint.\n",
      "Epoch 61, loss: 1.6982, f1_score: 0.4352, time: 7097 sec\n",
      "Epoch 62, loss: 1.6945, f1_score: 0.4362, time: 7058 sec\n",
      "Epoch 63, loss: 1.6915, f1_score: 0.4366, time: 7099 sec\n",
      "Epoch 64, loss: 1.6905, f1_score: 0.4360, time: 7109 sec\n",
      "Epoch 65, loss: 1.6865, f1_score: 0.4375, time: 7114 sec\n",
      "Saved checkpoint.\n",
      "Epoch 66, loss: 1.6851, f1_score: 0.4380, time: 7082 sec\n",
      "Epoch 67, loss: 1.6819, f1_score: 0.4381, time: 6648 sec\n",
      "Epoch 68, loss: 1.6793, f1_score: 0.4385, time: 7108 sec\n",
      "Epoch 69, loss: 1.6770, f1_score: 0.4394, time: 7058 sec\n",
      "Epoch 70, loss: 1.6764, f1_score: 0.4398, time: 7120 sec\n",
      "Saved checkpoint.\n",
      "Epoch 71, loss: 1.6728, f1_score: 0.4395, time: 7114 sec\n",
      "Epoch 72, loss: 1.6702, f1_score: 0.4405, time: 7131 sec\n",
      "Epoch 73, loss: 1.6686, f1_score: 0.4405, time: 7126 sec\n",
      "Epoch 74, loss: 1.6656, f1_score: 0.4408, time: 7095 sec\n",
      "Epoch 75, loss: 1.6657, f1_score: 0.4411, time: 7070 sec\n",
      "Saved checkpoint.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-4babe197b06e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtsImageFeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsQuestionFeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsAnswerLabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moTrainDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Make a training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mclGradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsImageFeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsQuestionFeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsAnswerLabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moOptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# tensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-597e6b3df3f6>\u001b[0m in \u001b[0;36mTrainStep\u001b[1;34m(oModel, tsImageFeature, tsQuestionFeature, tsAnswerLabel, oOptimizer)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Compute the gradient of the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mclGradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moGradientTape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Apply gradients to variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0moOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclGradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1078\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1079\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1080\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1732\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1734\u001b[1;33m     \u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1735\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1736\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5694\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m   5695\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5696\u001b[1;33m         transpose_b)\n\u001b[0m\u001b[0;32m   5697\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5698\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over all epochs\n",
    "for iEpoch in range(START_EPOCH, EPOCHS):\n",
    "    # time in seconds since the epoch as a floating point number\n",
    "    fStartTime = time.time()\n",
    "\n",
    "    # Loop over all elements in training dataset\n",
    "    for tsImageFeature, tsQuestionFeature, tsAnswerLabel in (oTrainDataset):\n",
    "        # Make a training step\n",
    "        clGradient = TrainStep(oModel, tsImageFeature, tsQuestionFeature, tsAnswerLabel, oOptimizer)\n",
    "\n",
    "    # tensorboard \n",
    "    # set default writer\n",
    "    with oTrainSummaryWriter.as_default():\n",
    "        # Write the crossentropy loss for later analysis in TensorBoard\n",
    "        tf.summary.scalar('loss', oTrainLoss.result(), step=iEpoch)\n",
    "        # Write the F1 score for later analysis in TensorBoard\n",
    "        tf.summary.scalar('f1_score', oTrainScore.result(), step=iEpoch)\n",
    "        # Create summaries to visualize weights\n",
    "        # Loop over all trainable variables\n",
    "        for tsVariable in oModel.trainable_variables:\n",
    "            # Writes a weights histogram for later analysis in TensorBoard\n",
    "            tf.summary.histogram(tsVariable.name, tsVariable, step=iEpoch)\n",
    "        # Summarize all gradients\n",
    "        # Loop over all gradients\n",
    "        for fGradient, tsVariable in clGradient:\n",
    "            # Writes a gradient histogram for later analysis in TensorBoard\n",
    "            tf.summary.histogram(tsVariable.name + '/gradient', fGradient, step=iEpoch)\n",
    "\n",
    "    sTemplateEpoch = 'Epoch {}, loss: {:.4f}, f1_score: {:.4f}, time: {:.0f} sec'\n",
    "    print (sTemplateEpoch.format(iEpoch + 1,\n",
    "                         oTrainLoss.result(), \n",
    "                         oTrainScore.result(),\n",
    "                         (time.time() - fStartTime)))\n",
    "\n",
    "    # Reset metric state variables every epoch\n",
    "    oTrainLoss.reset_states()\n",
    "    oTrainScore.reset_states()\n",
    "\n",
    "    # save checkpoint every SAVE_CKPT_FREQ step\n",
    "    # Add 1 to step\n",
    "    oCheckpoint.step.assign_add(1)\n",
    "    # After SAVE_CKPT_FREQ step\n",
    "    if int(oCheckpoint.step) % SAVE_CKPT_FREQ == 0:\n",
    "        # Save a checkpoint\n",
    "        oCheckPointManager.save()\n",
    "        print('Saved checkpoint.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378316f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88cce1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "oModel.save(f'{DATA_FOLDER}/TrainVal_CoAttention_VGG_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a6dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0f4e44d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b90567e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17264), started 14 days, 3:47:09 ago. (Use '!kill 17264' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f59126f4752c5adf\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f59126f4752c5adf\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir '../content/temps/logs/0.0001_256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66798e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
