{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efad5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "import time\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from prettytable import PrettyTable\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c09317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7b18a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Dropout, Embedding, concatenate, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing import text, image, sequence\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6463d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tf version: 2.5.0\n",
      "Using numpy version: 1.19.2\n"
     ]
    }
   ],
   "source": [
    "print('Using tf version: {}'.format(tf.__version__))\n",
    "print('Using numpy version: {}'.format(np.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d3bae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder of data\n",
    "DATA_FOLDER = '../content'\n",
    "# folder of processed data\n",
    "PROCESSED_DATA_FOLDER = '../content/processed_data'\n",
    "# folder of dataset evaluation\n",
    "EVALUATION_DATA_FOLDER = '../content/evaluation'\n",
    "# path image format\n",
    "IMAGE_FORMAT = '../content/%s/COCO_%s_%012d.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d96e3",
   "metadata": {},
   "source": [
    "# Preparing the data matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05b123b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading from disk\n",
    "with open(f'{PROCESSED_DATA_FOLDER}/vqa_raw_trainVal2014_top1000.json', 'rb') as f:\n",
    "    clQuestionTrainVal, clAnswerTrainVal, clAnswersTrainVal, clPathImageTrainVal = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9a7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk : question tokenization class\n",
    "with open(f'{DATA_FOLDER}/question_tokenizer.pkl', 'rb') as f:\n",
    "    oQuestionTok = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c64027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk : Tokenized questions\n",
    "with open(f'{DATA_FOLDER}/tokenised_question_paddingPost_train_val.pkl', mode='rb') as f:\n",
    "    caQuestionTrainValTokenized = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c9547a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk: Encode target answers with value between 0 and n_classes-1\n",
    "with open(f'{DATA_FOLDER}/AnswerEncoderTrainVal.pkl', 'rb') as f:\n",
    "    oAnswerEncoder = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e4a14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function name: GetAnswersMatrix\n",
    "    \n",
    "    Objective: One-hot-encode the answers\n",
    "    \n",
    "    Summary algorithmic description: Transform answers to normalized encoding.\n",
    "                                     Convert the normalized encoding to one-hot-coding\n",
    "    \n",
    "    Input parameters: clAnswer : list of answers\n",
    "                      cEncoder : LabelEncoder class\n",
    "    \n",
    "    Return : binary class matrix\n",
    "    \n",
    "    Date : 09/11/2021\n",
    "    \n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "def GetAnswersMatrix(clAnswer, cEncoder):\n",
    "    # Transform labels to normalized encoding.\n",
    "    y = cEncoder.transform(clAnswer) \n",
    "    # Number of classes \n",
    "    iNbClasses = cEncoder.classes_.shape[0]\n",
    "    # Convert a class vector (integers) to binary class matrix.\n",
    "    Y = utils.to_categorical(y, iNbClasses)\n",
    "    # Return the binary class matrix.\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2de741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot-encode the train answers \n",
    "caAnswerMatrix = GetAnswersMatrix(clAnswerTrainVal, oAnswerEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1162d333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(574913, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of answer matrix\n",
    "caAnswerMatrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697cbd61",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70cb9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "705af5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Class name: AttentionMaps \n",
    "    \n",
    "    Objective: Compute the image (or question) attention\n",
    "    \n",
    "    Summary algorithmic description: Calculate the affinity matrix\n",
    "                                     Predict image and question attention maps\n",
    "    \n",
    "    Input parameters: iDim_k : hidden attention dimention\n",
    "                      fRegValue : Regularization value\n",
    "    \n",
    "    Date : 14/11/2021\n",
    "    \n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "class AttentionMaps(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Function name: __init__\n",
    "\n",
    "        Objective: Define custom layer attributes\n",
    "\n",
    "        Summary algorithmic description: Define two attributes hidden attention dimention and regularization value\n",
    "                                         Define the weight parameters by layer Dense\n",
    "\n",
    "        Input parameters: iDim_k : hidden attention dimention\n",
    "                          fRegValue : Regularization value\n",
    "\n",
    "        Return : None\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def __init__(self, iDim_k, fRegValue, **kwargs):\n",
    "        # Default layer attributes\n",
    "        super(AttentionMaps, self).__init__(**kwargs)\n",
    "\n",
    "        # Get the value of hidden attention dimention\n",
    "        self.iDim_k = iDim_k\n",
    "        # Get the regularization value \n",
    "        self.fRegValue = fRegValue\n",
    "\n",
    "        # Dense layer with Xavier uniform initializer for image weight parameters \n",
    "        self.Wv = Dense(self.iDim_k, activation=None,\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.fRegValue),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=2))\n",
    "        # Dense layer with Xavier uniform initializer for question weight parameters\n",
    "        self.Wq = Dense(self.iDim_k, activation=None,\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.fRegValue),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=3))\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "        Function name: call\n",
    "\n",
    "        Objective: Perform the logic of applying the layer to the input tensors\n",
    "\n",
    "        Summary algorithmic description: Calculate the affinity matrix\n",
    "                                         Predict image and question attention maps\n",
    "\n",
    "        Input parameters: caImageFeat : image feature (b, N,  d) \n",
    "                          caQuestionFeat : question feature (b, T,  d)\n",
    "\n",
    "        Return : image attention map and question attention map\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def call(self, caImageFeat, caQuestionFeat):\n",
    "        # Affinity Matrix C = tanh((QT)(Wb)V) (b, T, N)\n",
    "        C = tf.matmul(caQuestionFeat, tf.transpose(caImageFeat, perm=[0,2,1])) \n",
    "        C = tf.keras.activations.tanh(C) \n",
    "\n",
    "        # (Wv)V (b, N, k)\n",
    "        Wv_V = self.Wv(caImageFeat)                             \n",
    "        # (Wq)Q (b, T, k)\n",
    "        Wq_Q = self.Wq(caQuestionFeat)                              \n",
    "\n",
    "        # ((Wq)Q)C (b, k, N)\n",
    "        Wq_Q_C = tf.matmul(tf.transpose(Wq_Q, perm=[0,2,1]), C) \n",
    "        # ((Wq)Q)C (b, N, k)\n",
    "        Wq_Q_C = tf.transpose(Wq_Q_C, perm =[0,2,1])       \n",
    "\n",
    "        # ((Wv)V)CT (b, k, T)                                         \n",
    "        Wv_V_CT = tf.matmul(tf.transpose(Wv_V, perm=[0,2,1]), tf.transpose(C, perm=[0,2,1])) \n",
    "        # ((Wv)V)CT (b, T, k) \n",
    "        Wv_V_CT = tf.transpose(Wv_V_CT, perm =[0,2,1])         \n",
    "\n",
    "        # image attention map Hv = tanh((Wv)V + ((Wq)Q)C) (b, N, k)\n",
    "        Hv = Wv_V + Wq_Q_C                                     \n",
    "        Hv = tf.keras.activations.tanh(Hv)              \n",
    "\n",
    "        # question attention map Hq = tanh((Wq)Q + ((Wv)V)CT) (b, T, k)\n",
    "        Hq = Wq_Q + Wv_V_CT                                     \n",
    "        Hq = tf.keras.activations.tanh(Hq)                   \n",
    "\n",
    "        # Return image attention map and question attention map\n",
    "        return [Hv, Hq]                                     \n",
    "  \n",
    "\n",
    "    \"\"\"\n",
    "        Function name: get_config\n",
    "\n",
    "        Objective: Collect the input shape and other information about the layer.\n",
    "\n",
    "        Summary algorithmic description: Return a dictionary containing the configuration used to initialize this layer\n",
    "\n",
    "        Input parameters: None\n",
    "\n",
    "        Return : a dictionary containing informations about the layer.\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def get_config(self):\n",
    "        # Dictionnary of two attributes of this layer\n",
    "        cdConfig = {\n",
    "            'iDim_k': self.iDim_k,\n",
    "            'fRegValue': self.fRegValue\n",
    "        }\n",
    "        # Dictionnary of base configurations\n",
    "        cdBasaConfig = super(AttentionMaps, self).get_config()\n",
    "        # Return a dictionary containing informations about the layer.\n",
    "        return dict(list(cdBasaConfig.items()) + list(cdConfig.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73f85dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'attention_maps', 'trainable': True, 'dtype': 'float32', 'iDim_k': 64, 'fRegValue': 0.001}\n"
     ]
    }
   ],
   "source": [
    "cLayerAttentionMaps = AttentionMaps(64, 0.001)\n",
    "cdConfig = cLayerAttentionMaps.get_config()\n",
    "print(cdConfig)\n",
    "cNewLayerAttentionMaps = AttentionMaps.from_config(cdConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba8bfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Class name: ContextVector \n",
    "    \n",
    "    Objective: Find context vector of the image and text features\n",
    "    \n",
    "    Summary algorithmic description: \n",
    "    \n",
    "    Input parameters: iDim_k : hidden attention dimention\n",
    "                      fRegValue : Regularization value\n",
    "    \n",
    "    Date : 14/11/2021\n",
    "    \n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "class ContextVector(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Function name: __init__\n",
    "\n",
    "        Objective: Define custom layer attributes\n",
    "\n",
    "        Summary algorithmic description: Define attribute regularization value\n",
    "                                         Define the weight parameters by layer Dense\n",
    "\n",
    "        Input parameters: fRegValue : Regularization value\n",
    "\n",
    "        Return : None\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def __init__(self, fRegValue, **kwargs):\n",
    "        # Default layer attributes\n",
    "        super(ContextVector, self).__init__(**kwargs)\n",
    "\n",
    "        # Get the regularization value \n",
    "        self.fRegValue = fRegValue\n",
    "        # Image weight parameters (Dense layer with Xavier uniform initializer)\n",
    "        self.whv = Dense(1, activation='softmax',\\\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(self.fRegValue),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=4))\n",
    "        # Image weight parameters (Dense layer with Xavier uniform initializer)\n",
    "        self.whq = Dense(1, activation='softmax',\\\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(self.fRegValue),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=5)) \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        Function name: call\n",
    "\n",
    "        Objective: Perform the logic of applying the layer to the input tensors\n",
    "\n",
    "        Summary algorithmic description: Find context vector of the image and text features\n",
    "\n",
    "        Input parameters: caImageFeat : image feature (b, N,  d) \n",
    "                          caQuestionFeat : question feature (b, T,  d)\n",
    "                          Hv : image attention map (b, N, k)\n",
    "                          Hq : question attention map (b, T, k)\n",
    "\n",
    "        Return : context vector of the image and text features\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def call(self, caImageFeat, caQuestionFeat, Hv, Hq):\n",
    "        # attention probabilities of each image region vn\n",
    "        # av = softmax(wT_hv * H_v) (b, N, 1)\n",
    "        av = self.whv(Hv)                               \n",
    "\n",
    "        # attention probabilities of each word qt      \n",
    "        # aq = softmax(wT_hq * H_q) (b, T, 1)\n",
    "        aq = self.whq(Hq)                              \n",
    "\n",
    "        # context vector for image (b, N, d)\n",
    "        v = av * caImageFeat \n",
    "        # context vector for image (b, d)\n",
    "        v = tf.reduce_sum(v, 1)                            \n",
    "\n",
    "        # context vector for question (b, T, d)\n",
    "        q = aq * caQuestionFeat                               \n",
    "        # context vector for question (b, d)\n",
    "        q = tf.reduce_sum(q, 1)            \n",
    "        \n",
    "        # Return context vector of the image and text features\n",
    "        return [v, q]\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "        Function name: get_config\n",
    "\n",
    "        Objective: Collect the input shape and other information about the layer.\n",
    "\n",
    "        Summary algorithmic description: Return a dictionary containing the configuration used to initialize this layer\n",
    "\n",
    "        Input parameters: None\n",
    "\n",
    "        Return : a dictionary containing informations about the layer.\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def get_config(self):\n",
    "        # Dictionnary of attributes of this layer\n",
    "        cdConfig = {\n",
    "            'fRegValue': self.fRegValue\n",
    "        }\n",
    "        # Dictionnary of base configurations\n",
    "        cdBaseConfig = super(ContextVector, self).get_config()\n",
    "        # Return a dictionary containing informations about the layer\n",
    "        return dict(list(cdBaseConfig.items()) + list(cdConfig.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a6f9ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'context_vector', 'trainable': True, 'dtype': 'float32', 'fRegValue': 0.001}\n"
     ]
    }
   ],
   "source": [
    "cLayerContextVector = ContextVector(0.001)\n",
    "cdConfig = cLayerContextVector.get_config()\n",
    "print(cdConfig)\n",
    "cNewLayerContextVector = ContextVector.from_config(cdConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8501c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Class name: PhraseLevelFeatures \n",
    "    \n",
    "    Objective: Compute the phrase features by applying 1-D convolution on the word embedding vectors\n",
    "    \n",
    "    Summary algorithmic description: Applying 1-D convolution with filters of three window sizes: unigram, bigram and trigram\n",
    "                                     Max-pooling across different n-grams at each word location\n",
    "    \n",
    "    Input parameters: iDim_d : hidden dimension\n",
    "    \n",
    "    Date : 14/11/2021\n",
    "    \n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "class PhraseLevelFeatures(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Function name: __init__\n",
    "\n",
    "        Objective: Define custom layer attributes\n",
    "\n",
    "        Summary algorithmic description: Define attribute hidden dimension\n",
    "                                         Define the weight parameters by 1D convolution layer\n",
    "\n",
    "        Input parameters: iDim_d : hidden dimension\n",
    "\n",
    "        Return : None\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def __init__(self, iDim_d, **kwargs):\n",
    "        # Default layer attributes\n",
    "        super(PhraseLevelFeatures, self).__init__(**kwargs)\n",
    "\n",
    "        # Get the hidden dimension\n",
    "        self.iDim_d = iDim_d\n",
    "        # Convolution with filter unigram\n",
    "        self.conv_unigram = Conv1D(self.iDim_d, kernel_size=1, strides=1,\\\n",
    "                                    kernel_initializer=tf.keras.initializers.glorot_uniform(seed=6)) \n",
    "        # Convolution with filter bigram\n",
    "        self.conv_bigram =  Conv1D(self.iDim_d, kernel_size=2, strides=1, padding='same',\\\n",
    "                                    kernel_initializer=tf.keras.initializers.glorot_uniform(seed=7)) \n",
    "        # Convolution with filter trigram\n",
    "        self.conv_trigram = Conv1D(self.iDim_d, kernel_size=3, strides=1, padding='same',\\\n",
    "                                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=8)) \n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "        Function name: call\n",
    "\n",
    "        Objective: Perform the logic of applying the layer to the input tensors\n",
    "\n",
    "        Summary algorithmic description: Compute the n-gram phrase embeddings (n = 1,2,3)\n",
    "\n",
    "        Input parameters: caWordFeat : word feature (b, T, d) \n",
    "\n",
    "        Return : Phrase level features of the question (b, T, d)\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def call(self, caWordFeat):\n",
    "        # phrase level unigram features (b, T, d)\n",
    "        qpUnigram = self.conv_unigram(caWordFeat)                    \n",
    "\n",
    "        # phrase level bigram features (b, T, d)\n",
    "        qpBigram  = self.conv_bigram(caWordFeat)                     \n",
    "\n",
    "        # phrase level trigram features (b, T, d)\n",
    "        qpTrigram = self.conv_trigram(caWordFeat)             \n",
    "\n",
    "        # phrase level features (b, T, d, 3)\n",
    "        qp = tf.concat([tf.expand_dims(qpUnigram, -1),\\\n",
    "                        tf.expand_dims(qpBigram, -1),\\\n",
    "                        tf.expand_dims(qpTrigram, -1)], -1)         \n",
    "\n",
    "        # Max-pool across n-gram features, over-all phrase level feature (b, T, d)\n",
    "        qp = tf.reduce_max(qp, -1)                                \n",
    "\n",
    "        # Return phrase level features\n",
    "        return qp\n",
    "\n",
    "    \"\"\"\n",
    "        Function name: get_config\n",
    "\n",
    "        Objective: Collect the input shape and other information about the layer.\n",
    "\n",
    "        Summary algorithmic description: Return a dictionary containing the configuration used to initialize this layer\n",
    "\n",
    "        Input parameters: None\n",
    "\n",
    "        Return : a dictionary containing informations about the layer.\n",
    "\n",
    "        Date : 14/11/2021\n",
    "\n",
    "        Coding: INSA CVL - Van Tuan BUI  \n",
    "    \"\"\"\n",
    "    def get_config(self):\n",
    "        # Dictionnary of attributes of this layer\n",
    "        cdConfig = {\n",
    "            'iDim_d': self.iDim_d\n",
    "        }\n",
    "        # Dictionnary of base configurations\n",
    "        cdBaseConfig = super(PhraseLevelFeatures, self).get_config()\n",
    "        # Return a dictionary containing informations about the layer\n",
    "        return dict(list(cdBaseConfig.items()) + list(cdConfig.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22e70939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'phrase_level_features', 'trainable': True, 'dtype': 'float32', 'iDim_d': 32}\n"
     ]
    }
   ],
   "source": [
    "cLayerPhraseLevelFeatures = PhraseLevelFeatures(32)\n",
    "cdConfig = cLayerPhraseLevelFeatures.get_config()\n",
    "print(cdConfig)\n",
    "cNewLayerPhraseLevelFeatures = PhraseLevelFeatures.from_config(cdConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bfcab",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d6c2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function name: BuildModel\n",
    "\n",
    "    Objective: build the training model\n",
    "\n",
    "    Summary algorithmic description: Calculate co-attended image and question features from all three levels\n",
    "                                     Use a multi-layer perceptron (MLP) to recursively encode the attention features\n",
    "    \n",
    "    Input parameters: iMaxAnswers : Number of output targets of the model\n",
    "                      iMaxSequenceLength : Maximum length of input sequences\n",
    "                      iVocabSize : Size of the vocabulary\n",
    "                      iDim_d : Hidden dimension\n",
    "                      iDim_k : Hidden attention dimension\n",
    "                      fLearningRate : Learning rate for the model\n",
    "                      fDropoutRate : Dropout rate\n",
    "                      fRegValue : Regularization value\n",
    "\n",
    "    Return : The training model\n",
    "\n",
    "    Date : 14/11/2021\n",
    "\n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "def BuildModel(iMaxAnswers, iMaxSequenceLength, iVocabSize, iDim_d, iDim_k, fLearningRate, fDropoutRate, fRegValue):\n",
    "    # inputs \n",
    "    caImageInput = Input(shape=(49, 2048, ), name='Image_Input')\n",
    "    caQuestionInput = Input(shape=(22, ), name='Question_Input')\n",
    "\n",
    "    # image feature (Wb)V (b, N, d)\n",
    "    caImageFeature = Dense(iDim_d, activation=None, name='Image_Feat_Dense',\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(fRegValue),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=1))(caImageInput)\n",
    "    # image feature (b, N, d)\n",
    "    caImageFeature = Dropout(fDropoutRate, seed=1)(caImageFeature)\n",
    "\n",
    "    # word embedding feature\n",
    "    caWordFeature = Embedding(input_dim=iVocabSize, output_dim=iDim_d, input_length=iMaxSequenceLength,\\\n",
    "                            mask_zero=True)(caQuestionInput)\n",
    "    # image and question attention maps at word level\n",
    "    Hvw, Hqw = AttentionMaps(iDim_k, fRegValue, name='AttentionMaps_Word')(caImageFeature, caWordFeature)\n",
    "    # context vector of the image and text features at word level\n",
    "    vw, qw = ContextVector(fRegValue, name='ContextVector_Word')(caImageFeature, caWordFeature, Hvw, Hqw)\n",
    "    # attention features at word level\n",
    "    caWordAttentionFeature = tf.add(vw, qw)\n",
    "    # co-attended image and question features from word level\n",
    "    hw = Dense(iDim_d, activation='tanh', name='hw_Dense',\\\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(fRegValue),\\\n",
    "                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=13))(caWordAttentionFeature)\n",
    "\n",
    "    # phrase level features\n",
    "    caPhraseFeature = PhraseLevelFeatures(iDim_d, name='PhraseLevelFeatures')(caWordFeature)\n",
    "    # image and question attention maps at phrase level\n",
    "    Hvp, Hqp = AttentionMaps(iDim_k, fRegValue, name='AttentionMaps_Phrase')(caImageFeature, caPhraseFeature)\n",
    "    # context vector of the image and text features at phrase level\n",
    "    vp, qp = ContextVector(fRegValue, name='ContextVector_Phrase')(caImageFeature, caPhraseFeature, Hvp, Hqp)\n",
    "    # attention features at phrase level\n",
    "    caPhraseAttentionFeature = concatenate([tf.add(vp, qp), hw], -1) \n",
    "    # co-attended image and question features from phrase level\n",
    "    hp = Dense(iDim_d, activation='tanh', name='hp_Dense',\\\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(fRegValue),\\\n",
    "                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=14))(caPhraseAttentionFeature)\n",
    "\n",
    "    # sentence level\n",
    "    caSentenceFeature = LSTM(iDim_d, return_sequences=True, input_shape=(None, iMaxSequenceLength, iDim_d),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(caPhraseFeature)\n",
    "    # image and question attention maps at sentence level\n",
    "    Hvs, Hqs = AttentionMaps(iDim_k, fRegValue, name='AttentionMaps_Sent')(caImageFeature, caSentenceFeature)\n",
    "    # context vector of the image and text features at sentence level\n",
    "    vs, qs = ContextVector(fRegValue, name='ContextVector_Sent')(caImageFeature, caSentenceFeature, Hvs, Hqs)\n",
    "    # attention features at sentence level\n",
    "    caSentenceAttentionFeature = concatenate([tf.add(vs, qs), hp], -1) \n",
    "    # co-attended image and question features from sentence level\n",
    "    hs = Dense(2 * iDim_d, activation='tanh', name='hs_Dense',\\\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(fRegValue),\\\n",
    "                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=15))(caSentenceAttentionFeature)\n",
    "\n",
    "    # encode the attention features\n",
    "    z = Dense(2 * iDim_d, activation='tanh', name='z_Dense',\\\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(fRegValue),\\\n",
    "                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(hs)\n",
    "    # encode the attention features\n",
    "    z = Dropout(fDropoutRate, seed=16)(z)\n",
    "\n",
    "    # probability of the final answer\n",
    "    result = Dense(iMaxAnswers, activation='softmax')(z)\n",
    "    \n",
    "    # group layers\n",
    "    model = Model(inputs=[caImageInput, caQuestionInput], outputs=result)\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2932556",
   "metadata": {},
   "source": [
    "# Create tf.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "181febf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 300\n",
    "# buffer siez\n",
    "BUFFER_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70f30a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function name: GetImageFeatures\n",
    "\n",
    "    Objective: Load the image features from numpy files\n",
    "\n",
    "    Summary algorithmic description: Load the image features from numpy files\n",
    "\n",
    "    Input parameters: sPathImage : path image\n",
    "                      caQuestion : Tokenization of question\n",
    "                      caAnswer : answer one-hot-enconding \n",
    "\n",
    "    Return : caImageTensor : image features\n",
    "             caQuestion : Tokenization of question\n",
    "             caAnswer : answer one-hot-enconding \n",
    "\n",
    "    Date : 15/11/2021\n",
    "\n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "def GetImageFeatures(sPathImage, caQuestion, caAnswer):\n",
    "    # Load the image features from numpy files\n",
    "    caImageTensor = np.load(f'{DATA_FOLDER}/featuresResNet50/' + sPathImage.decode('utf-8').split('.')[2][-6:] + '.npy')\n",
    "    # Return image features, tokenization of question and answer one-hot-enconding \n",
    "    return caImageTensor, caQuestion, caAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b3c8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset whose elements are slices along their first dimension\n",
    "oTrainDataset = tf.data.Dataset.from_tensor_slices((clPathImageTrainVal, caQuestionTrainValTokenized, caAnswerMatrix))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "oTrainDataset = oTrainDataset.map(lambda sPathImage, caQuestion, caAnswer: tf.numpy_function(\n",
    "    GetImageFeatures, [sPathImage, caQuestion, caAnswer], [tf.float32, tf.int32, tf.float32]),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "oTrainDataset = oTrainDataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# allow later elements to be prepared while the current element is being processed.\n",
    "oTrainDataset = oTrainDataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354a0ff",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97702b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params 1\n",
    "MAX_ANSWERS = 1000\n",
    "MAX_SEQUENCE_LENGTH = 22\n",
    "VOCAB_SIZE = len(oQuestionTok.word_index) + 1\n",
    "EPOCHS      = 200\n",
    "DIM_D = 512\n",
    "DIM_K = 256\n",
    "LEARNING_RATE = 1e-4\n",
    "DROPOUT_RATE = 0.5\n",
    "REG_VALUE = 0.01\n",
    "\n",
    "BASE_PATH = f'{DATA_FOLDER}/temps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bad53ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17984"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ffdfd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "oModel = BuildModel(MAX_ANSWERS, MAX_SEQUENCE_LENGTH, VOCAB_SIZE, DIM_D, DIM_K, LEARNING_RATE, DROPOUT_RATE, REG_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f3006f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Image_Input (InputLayer)        [(None, 49, 2048)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Image_Feat_Dense (Dense)        (None, 49, 512)      1049088     Image_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Question_Input (InputLayer)     [(None, 22)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 49, 512)      0           Image_Feat_Dense[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 22, 512)      9207808     Question_Input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "PhraseLevelFeatures (PhraseLeve (None, 22, 512)      1574400     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "AttentionMaps_Word (AttentionMa [(None, 49, 256), (N 262656      dropout[0][0]                    \n",
      "                                                                 embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "AttentionMaps_Phrase (Attention [(None, 49, 256), (N 262656      dropout[0][0]                    \n",
      "                                                                 PhraseLevelFeatures[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ContextVector_Word (ContextVect [(None, 512), (None, 514         dropout[0][0]                    \n",
      "                                                                 embedding[0][0]                  \n",
      "                                                                 AttentionMaps_Word[0][0]         \n",
      "                                                                 AttentionMaps_Word[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 22, 512)      2099200     PhraseLevelFeatures[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ContextVector_Phrase (ContextVe [(None, 512), (None, 514         dropout[0][0]                    \n",
      "                                                                 PhraseLevelFeatures[0][0]        \n",
      "                                                                 AttentionMaps_Phrase[0][0]       \n",
      "                                                                 AttentionMaps_Phrase[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add (TFOpLambda)        (None, 512)          0           ContextVector_Word[0][0]         \n",
      "                                                                 ContextVector_Word[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "AttentionMaps_Sent (AttentionMa [(None, 49, 256), (N 262656      dropout[0][0]                    \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_1 (TFOpLambda)      (None, 512)          0           ContextVector_Phrase[0][0]       \n",
      "                                                                 ContextVector_Phrase[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "hw_Dense (Dense)                (None, 512)          262656      tf.math.add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ContextVector_Sent (ContextVect [(None, 512), (None, 514         dropout[0][0]                    \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 AttentionMaps_Sent[0][0]         \n",
      "                                                                 AttentionMaps_Sent[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           tf.math.add_1[0][0]              \n",
      "                                                                 hw_Dense[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_2 (TFOpLambda)      (None, 512)          0           ContextVector_Sent[0][0]         \n",
      "                                                                 ContextVector_Sent[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "hp_Dense (Dense)                (None, 512)          524800      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1024)         0           tf.math.add_2[0][0]              \n",
      "                                                                 hp_Dense[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hs_Dense (Dense)                (None, 1024)         1049600     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_Dense (Dense)                 (None, 1024)         1049600     hs_Dense[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           z_Dense[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1000)         1025000     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 18,631,662\n",
      "Trainable params: 18,631,662\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "oModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43906c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of steps per epoch\n",
    "iStepsPerEpoch = int(np.ceil(len(clPathImageTrainVal)/BATCH_SIZE))\n",
    "# interval boundaries for changed learning rates.\n",
    "clBoundary = [50 * iStepsPerEpoch]\n",
    "# the learning rate values for the intervals defined by boundaries.\n",
    "clLearningRateValue = [LEARNING_RATE, LEARNING_RATE / 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d059ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reduce the learning rate after 50th epoch (from 1e-4 to 1e-5)\n",
    "learningRateSchedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(clBoundary, clLearningRateValue)\n",
    "# Optimizer that implements the Adam algorithm\n",
    "oOptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "# the crossentropy loss between the labels and predictions\n",
    "oLoss = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b933c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint directory\n",
    "sCheckpointDirectory = BASE_PATH + \"/training_checkpoints/\" + str(LEARNING_RATE) + \"_\" + str(DIM_K)\n",
    "# save checkpoint every SAVE_CKPT_FREQ step\n",
    "SAVE_CKPT_FREQ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1c94ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "# Create a Checkpoint that will manage three objects with trackable state\n",
    "oCheckpoint = tf.train.Checkpoint(step=tf.Variable(0), optimizer=oOptimizer, model=oModel)\n",
    "# keep only 3 newest checkpoints \n",
    "oCheckPointManager = tf.train.CheckpointManager(oCheckpoint, sCheckpointDirectory, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd0c5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the (weighted) mean of the given values\n",
    "oTrainLoss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2861196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F-1 Score\n",
    "oTrainScore = F1Score(num_classes=MAX_ANSWERS, average='micro', name='train_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1c771fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training log directory\n",
    "sTrainLogDir = BASE_PATH + '/logs/' + str(LEARNING_RATE) + \"_\" + str(DIM_K) + '/train'\n",
    "\n",
    "# Create a summary file writer for the training log directory\n",
    "oTrainSummaryWriter = tf.summary.create_file_writer(sTrainLogDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "10516af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function name: TrainStep\n",
    "\n",
    "    Objective: Operate a traning step\n",
    "\n",
    "    Summary algorithmic description: Make a forward pass\n",
    "                                     Make a backward pass\n",
    "                                     Record results : loss mean, F1 score\n",
    "\n",
    "    Input parameters: oModel : training model\n",
    "                      tsImageFeature : image feature tensor\n",
    "                      tsQuestionFeature : question feature tensor\n",
    "                      tsAnswerLabel : answer label tensor\n",
    "                      oOptimizer : Optimizer object\n",
    "\n",
    "    Return : clGradient_ : all gradients against trainable variables\n",
    "\n",
    "    Date : 15/11/2021\n",
    "\n",
    "    Coding: INSA CVL - Van Tuan BUI  \n",
    "\"\"\"\n",
    "def TrainStep(oModel, tsImageFeature, tsQuestionFeature, tsAnswerLabel, oOptimizer):\n",
    "    # Record operations for automatic differentiation\n",
    "    with tf.GradientTape() as oGradientTape:\n",
    "        # forward pass\n",
    "        # résultat de la prédiction\n",
    "        tsPrediction = oModel([tsImageFeature, tsQuestionFeature], training=True)\n",
    "        # the crossentropy loss\n",
    "        fLoss = oLoss(tsAnswerLabel, tsPrediction)\n",
    "\n",
    "    # backward pass\n",
    "    # Compute the gradient of the loss\n",
    "    clGradient = oGradientTape.gradient(fLoss, oModel.trainable_variables)\n",
    "    # Apply gradients to variables\n",
    "    oOptimizer.apply_gradients(zip(clGradient, oModel.trainable_variables))\n",
    "\n",
    "    # record results\n",
    "    # Loss Mean\n",
    "    oTrainLoss(fLoss)\n",
    "    # F-1 Score\n",
    "    oTrainScore(tsAnswerLabel, tsPrediction)\n",
    "\n",
    "    # all gradients\n",
    "    clGradient_ = list(zip(clGradient, oModel.trainable_variables))\n",
    "    # Return all gradients\n",
    "    return clGradient_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "66dd09aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from ../content/temps/training_checkpoints/0.0001_256\\ckpt-9\n",
      "Resume training from epoch: 45\n"
     ]
    }
   ],
   "source": [
    "# If the prefix of the most recent checkpoint exist\n",
    "if oCheckPointManager.latest_checkpoint:\n",
    "    # Restore the latest checkpoint\n",
    "    oCheckpoint.restore(oCheckPointManager.latest_checkpoint)\n",
    "    # Display the prefix of the most recent checkpoint\n",
    "    print(\"Restored from {}\".format(oCheckPointManager.latest_checkpoint))\n",
    "    # latest training epoch\n",
    "    START_EPOCH = int(oCheckPointManager.latest_checkpoint.split('-')[-1]) * SAVE_CKPT_FREQ\n",
    "    print(\"Resume training from epoch: {}\".format(START_EPOCH))\n",
    "# If the prefix of the most recent checkpoint doesn't exist\n",
    "else:\n",
    "    print(\"Initializing from scratch\")\n",
    "    # Train from scratch\n",
    "    START_EPOCH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "614b48a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, loss: 1.4928, f1_score: 0.4776, time: 6570 sec\n",
      "Epoch 47, loss: 1.4874, f1_score: 0.4788, time: 6801 sec\n",
      "Epoch 48, loss: 1.4843, f1_score: 0.4801, time: 6977 sec\n",
      "Epoch 49, loss: 1.4779, f1_score: 0.4815, time: 7089 sec\n",
      "Epoch 50, loss: 1.4768, f1_score: 0.4819, time: 7178 sec\n",
      "Saved checkpoint.\n",
      "Epoch 51, loss: 1.4718, f1_score: 0.4837, time: 7616 sec\n",
      "Epoch 52, loss: 1.4704, f1_score: 0.4843, time: 7684 sec\n",
      "Epoch 53, loss: 1.4686, f1_score: 0.4841, time: 7830 sec\n",
      "Epoch 54, loss: 1.4607, f1_score: 0.4872, time: 8081 sec\n",
      "Epoch 55, loss: 1.4601, f1_score: 0.4883, time: 8134 sec\n",
      "Saved checkpoint.\n",
      "Epoch 56, loss: 1.4508, f1_score: 0.4890, time: 8352 sec\n",
      "Epoch 57, loss: 1.4534, f1_score: 0.4893, time: 8613 sec\n",
      "Epoch 58, loss: 1.4463, f1_score: 0.4905, time: 8884 sec\n",
      "Epoch 59, loss: 1.4422, f1_score: 0.4913, time: 8915 sec\n",
      "Epoch 60, loss: 1.4394, f1_score: 0.4926, time: 9281 sec\n",
      "Saved checkpoint.\n",
      "Epoch 61, loss: 1.4370, f1_score: 0.4943, time: 9248 sec\n",
      "Epoch 62, loss: 1.4354, f1_score: 0.4939, time: 9338 sec\n",
      "Epoch 63, loss: 1.4321, f1_score: 0.4954, time: 9416 sec\n",
      "Epoch 64, loss: 1.4293, f1_score: 0.4969, time: 9695 sec\n",
      "Epoch 65, loss: 1.4244, f1_score: 0.4966, time: 9549 sec\n",
      "Saved checkpoint.\n",
      "Epoch 66, loss: 1.4201, f1_score: 0.4983, time: 9377 sec\n",
      "Epoch 67, loss: 1.4225, f1_score: 0.4976, time: 9407 sec\n",
      "Epoch 68, loss: 1.4140, f1_score: 0.4992, time: 9467 sec\n",
      "Epoch 69, loss: 1.4144, f1_score: 0.4995, time: 9528 sec\n",
      "Epoch 70, loss: 1.4131, f1_score: 0.5008, time: 10321 sec\n",
      "Saved checkpoint.\n",
      "Epoch 71, loss: 1.4088, f1_score: 0.5014, time: 9580 sec\n",
      "Epoch 72, loss: 1.4095, f1_score: 0.5021, time: 9772 sec\n",
      "Epoch 73, loss: 1.4032, f1_score: 0.5028, time: 9868 sec\n",
      "Epoch 74, loss: 1.4017, f1_score: 0.5035, time: 9708 sec\n",
      "Epoch 75, loss: 1.4019, f1_score: 0.5040, time: 9711 sec\n",
      "Saved checkpoint.\n",
      "Epoch 76, loss: 1.3958, f1_score: 0.5049, time: 9812 sec\n",
      "Epoch 77, loss: 1.3944, f1_score: 0.5057, time: 10093 sec\n",
      "Epoch 78, loss: 1.3918, f1_score: 0.5071, time: 9931 sec\n",
      "Epoch 79, loss: 1.3877, f1_score: 0.5066, time: 9963 sec\n",
      "Epoch 80, loss: 1.3906, f1_score: 0.5066, time: 10041 sec\n",
      "Saved checkpoint.\n",
      "Epoch 81, loss: 1.3892, f1_score: 0.5074, time: 10222 sec\n",
      "Epoch 82, loss: 1.3890, f1_score: 0.5068, time: 10142 sec\n",
      "Epoch 83, loss: 1.3848, f1_score: 0.5083, time: 10153 sec\n",
      "Epoch 84, loss: 1.3824, f1_score: 0.5085, time: 10163 sec\n",
      "Epoch 85, loss: 1.3825, f1_score: 0.5090, time: 10214 sec\n",
      "Saved checkpoint.\n",
      "Epoch 86, loss: 1.3775, f1_score: 0.5100, time: 10232 sec\n",
      "Epoch 87, loss: 1.3762, f1_score: 0.5109, time: 10339 sec\n",
      "Epoch 88, loss: 1.3755, f1_score: 0.5116, time: 10667 sec\n",
      "Epoch 89, loss: 1.3736, f1_score: 0.5112, time: 10420 sec\n",
      "Epoch 90, loss: 1.3747, f1_score: 0.5112, time: 10292 sec\n",
      "Saved checkpoint.\n",
      "Epoch 91, loss: 1.3734, f1_score: 0.5119, time: 10302 sec\n",
      "Epoch 92, loss: 1.3701, f1_score: 0.5120, time: 10272 sec\n",
      "Epoch 93, loss: 1.3709, f1_score: 0.5123, time: 10322 sec\n",
      "Epoch 94, loss: 1.3671, f1_score: 0.5128, time: 10355 sec\n",
      "Epoch 95, loss: 1.3670, f1_score: 0.5140, time: 10393 sec\n",
      "Saved checkpoint.\n",
      "Epoch 96, loss: 1.3643, f1_score: 0.5146, time: 11177 sec\n",
      "Epoch 97, loss: 1.3607, f1_score: 0.5164, time: 11103 sec\n",
      "Epoch 98, loss: 1.3611, f1_score: 0.5154, time: 10883 sec\n",
      "Epoch 99, loss: 1.3597, f1_score: 0.5163, time: 10912 sec\n",
      "Epoch 100, loss: 1.3587, f1_score: 0.5159, time: 10878 sec\n",
      "Saved checkpoint.\n",
      "Epoch 101, loss: 1.3568, f1_score: 0.5163, time: 10565 sec\n",
      "Epoch 102, loss: 1.3595, f1_score: 0.5159, time: 10501 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-4babe197b06e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtsImageFeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsQuestionFeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsAnswerLabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moTrainDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Make a training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mclGradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsImageFeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsQuestionFeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsAnswerLabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moOptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# tensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-69-597e6b3df3f6>\u001b[0m in \u001b[0;36mTrainStep\u001b[1;34m(oModel, tsImageFeature, tsQuestionFeature, tsAnswerLabel, oOptimizer)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# résultat de la prédiction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mtsPrediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtsImageFeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsQuestionFeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;31m# the crossentropy loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mfLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtsAnswerLabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsPrediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1030\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    419\u001b[0m     \"\"\"\n\u001b[0;32m    420\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m--> 421\u001b[1;33m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1030\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-292d61cb7f77>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, caWordFeat)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# phrase level bigram features (b, T, d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mqpBigram\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_bigram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaWordFeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# phrase level trigram features (b, T, d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1030\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    247\u001b[0m       \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1017\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m       name=name)\n\u001b[0m\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[0;32m   1147\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m   1150\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in a future version'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[1;32m--> 602\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in a future version'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[1;32m--> 602\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv1d\u001b[1;34m(value, filters, stride, padding, use_cudnn_on_gpu, data_format, name, input, dilations)\u001b[0m\n\u001b[0;32m   1905\u001b[0m           \u001b[0minner_rank\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1906\u001b[0m           name=name)\n\u001b[1;32m-> 1907\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspatial_start_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m                 instructions)\n\u001b[1;32m--> 535\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36msqueeze\u001b[1;34m(input, axis, name, squeeze_dims)\u001b[0m\n\u001b[0;32m   4469\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4470\u001b[0m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4471\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\VQA\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36msqueeze\u001b[1;34m(input, axis, name)\u001b[0m\n\u001b[0;32m  10170\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10171\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m> 10172\u001b[1;33m         _ctx, \"Squeeze\", name, input, \"squeeze_dims\", axis)\n\u001b[0m\u001b[0;32m  10173\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10174\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over all epochs\n",
    "for iEpoch in range(START_EPOCH, EPOCHS):\n",
    "    # time in seconds since the epoch as a floating point number\n",
    "    fStartTime = time.time()\n",
    "\n",
    "    # Loop over all elements in training dataset\n",
    "    for tsImageFeature, tsQuestionFeature, tsAnswerLabel in (oTrainDataset):\n",
    "        # Make a training step\n",
    "        clGradient = TrainStep(oModel, tsImageFeature, tsQuestionFeature, tsAnswerLabel, oOptimizer)\n",
    "\n",
    "    # tensorboard \n",
    "    # set default writer\n",
    "    with oTrainSummaryWriter.as_default():\n",
    "        # Write the crossentropy loss for later analysis in TensorBoard\n",
    "        tf.summary.scalar('loss', oTrainLoss.result(), step=iEpoch)\n",
    "        # Write the F1 score for later analysis in TensorBoard\n",
    "        tf.summary.scalar('f1_score', oTrainScore.result(), step=iEpoch)\n",
    "        # Create summaries to visualize weights\n",
    "        # Loop over all trainable variables\n",
    "        for tsVariable in oModel.trainable_variables:\n",
    "            # Writes a weights histogram for later analysis in TensorBoard\n",
    "            tf.summary.histogram(tsVariable.name, tsVariable, step=iEpoch)\n",
    "        # Summarize all gradients\n",
    "        # Loop over all gradients\n",
    "        for fGradient, tsVariable in clGradient:\n",
    "            # Writes a gradient histogram for later analysis in TensorBoard\n",
    "            tf.summary.histogram(tsVariable.name + '/gradient', fGradient, step=iEpoch)\n",
    "\n",
    "    sTemplateEpoch = 'Epoch {}, loss: {:.4f}, f1_score: {:.4f}, time: {:.0f} sec'\n",
    "    print (sTemplateEpoch.format(iEpoch + 1,\n",
    "                         oTrainLoss.result(), \n",
    "                         oTrainScore.result(),\n",
    "                         (time.time() - fStartTime)))\n",
    "\n",
    "    # Reset metric state variables every epoch\n",
    "    oTrainLoss.reset_states()\n",
    "    oTrainScore.reset_states()\n",
    "\n",
    "    # save checkpoint every SAVE_CKPT_FREQ step\n",
    "    # Add 1 to step\n",
    "    oCheckpoint.step.assign_add(1)\n",
    "    # After SAVE_CKPT_FREQ step\n",
    "    if int(oCheckpoint.step) % SAVE_CKPT_FREQ == 0:\n",
    "        # Save a checkpoint\n",
    "        oCheckPointManager.save()\n",
    "        print('Saved checkpoint.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378316f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88cce1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "oModel.save(f'{DATA_FOLDER}/TrainVal_CoAttention_Resnet50_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a6dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f4e44d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b90567e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1adf565ed97ca728\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1adf565ed97ca728\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir '../content/temps/logs/0.0001_256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66798e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
